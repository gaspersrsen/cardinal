# Multi-Group Cross Section Generation

In this tutorial, you will learn how to generate [!ac](MGXS) with distributed cell tallies to couple Cardinal with
deterministic transport codes. To access this tutorial,

```
cd cardinal/tutorials/lwr_mgxs
```

!alert! note title=Previous Experience
In this tutorial we assume that the user is familiar with mesh generation with the
[Reactor module](https://mooseframework.inl.gov/modules/reactor/index.html) and running
[!ac](LWR) calculations with Cardinal-OpenMC. The [LWR AMR tutorial](openmc_amr.md) is
used as a base for this tutorial; we recommend it for users which are not familiar with
these concepts.
!alert-end!

## Geometry and Computational Models id=model_1

!include c5g7_ce.md

When generating [!ac](MGXS) tallies do not need to be normalized to power, but Cardinal
still requires a value of reactor power when performing k-eigenvalue calculations. For this tutorial we
select an arbitrary power of 1 Wth, however if you want to compute cross sections which take into account
multi-physics feedback a true reactor power will be required (alongside a coupled thermal-hydraulics solve).

### OpenMC Model id=openmc

The OpenMC model follows standard model building practices for [!ac](LWR) geometries. The Python script used
to generate the `model.xml` file can be found below, and a plot of the geometry can be found in [assembly_openmc].

!listing /tutorials/lwr_amr/make_openmc_model.py

!media assembly_amr_openmc.png
  id=assembly_openmc
  caption=OpenMC geometry colored by material ID shown on the $x$-$y$ and $x$-$z$ planes
  style=width:90%;margin-left:auto;margin-right:auto

To generate the XML files needed to run OpenMC, you can run the following:

```bash
python make_openmc_model.py
```

Or you can use the `model.xml` file that is included in the `tutorials/lwr_mgxs` directory.

### Mesh Mirror for MGXS Generation id=mesh

Most [!ac](MGXS) workflows store the resulting cross sections in custom libraries for specific deterministic
transport codes. As opposed to this workflow, Cardinal determines it's homogenization volumes using the mesh
mirror, and stores the cross sections on the mesh at the end of the OpenMC simulation. This allows for the
"transfer" of [!ac](MGXS) between Cardinal and a coupled deterministic code using the multi-app system, enabling
the use of higher fidelity cross sections and the recalculation of group properties to take multi-physics feedback
into account. The input file used to generate this mesh mirror can be found below.

!listing /tutorials/lwr_mgxs/mesh.i

The mesh is similar to the mesh generated in the [AMR tutorial](openmc_amr.md), with a few key differences to
accomodate a coupled deterministic transport code. We set `NUM_SECTORS` and `FUEL_RADIAL_DIVISIONS` to `4` to increase
the fidelity of the radial discretization of each pin. This is necessary for deterministic calculations as these regions
will have strong gradients due to absorption in the fuel and control rods. We also avoid deleting the gap blocks and
extruded the mesh an additional 21.42 cm to cover the reflector. These changes are necessary to preserve mesh connectivity
for deterministic transport solvers that want to use these cross sections, and to ensure we generate cross sections for
the reflector above the core.

!listing /tutorials/lwr_mgxs/mesh.i
  start=[3D_Core]
  end=[Rename]

The input mesh Cardinal will use to store cell tally results on (`mesh_in.e`) can be generated by running the
command below. The resulting mesh can be found in [mgxs_mesh]; we note that this mesh is substantially finer
in then a conventional Cardinal mesh mirror due to the need to adequately capture spatial gradients in the determistic solver.

```bash
cardinal-opt -i mesh.i --mesh-only
```

!media mgxs_mesh.png
  id=mgxs_mesh
  caption=The mesh mirror used for computing and storing [!ac](MGXS), shown with an isometric view and sliced on the $x-y$ plane
  style=width:90%;margin-left:auto;margin-right:auto

### Neutronics Input File id=neutronics

The neutronics calculation is performed over the entire assembly by OpenMC, and the wrapping of the OpenMC results in MOOSE is performed in
`openmc_mgxs.i`. We begin by defining the mesh that will be used by Cardinal to determine which cell tallies should be constructed for [!ac](MGXS)
generation, which was generated in the previous step:

!listing /tutorials/lwr_mgxs/openmc_mgxs.i
  block=Mesh

Afterwards, we add two auxvariables and auxkernels for post-processing the cross sections - we uses these for computing the scattering ratio
(the sum of scattering cross sections divided by the total cross section) in each energy group. The scattering ratio for each group should
be less than or equal to one; scattering ratios larger than one may result in non-convergence of the deterministic solver or convergence to
incorrect results. Note that we did not delete the gap blocks in the mesh, but we are block restricting the auxkernels and auxvariables such that
they are not computed over the gap region (block 2). This is necessary as the gaps will not receive any tally hits in the OpenMC, and so the
cross sections will be zero (resulting in a divide by zero when computing scattering ratios).

!listing /tutorials/lwr_mgxs/openmc_mgxs.i
  block=AuxVariables AuxKernels

Next, the [Problem block](Problem/index.md) (with an [OpenMCCellAverageProblem](OpenMCCellAverageProblem.md)) describes the syntax necessary to
replace the normal MOOSE finite element calculation with an OpenMC neutronics solve. We select `cell_level = 1` to ensure that we generate our
cell to element mapping for an OpenMC geometry depth one lower than the root universe. This will result in a distributed cell tally for each unique
geometric region in the [!ac](LWR) lattice, as opposed to the entire lattice (which is what would happen with `cell_level = 0`). We then specify the number
of particles (`particles = 10000`), the number of inactive batches (`inactive_batches = 50`), and the total number of batches (`batches = 1000`).

!listing /tutorials/lwr_mgxs/openmc_mgxs.i
  start=[Problem]
  end=[Problem/MGXS]

After specifying the basis problem settings, we move on to setting up the [!ac](MGXS) parameters with the [MGXS block](SetupMGXSAction.md). We specify
that a distributed cell tally should be used as the [!ac](MGXS) homogenization domain with `tally_type = cell` and the CASMO 2 group structure be used
for the energy group boundaries. We then select `particle = neutron` to set the particle filter to use when computing cross sections, which proves
to be useful when the OpenMC model is performing coupled neutron-photon transport. We then specify `estimator = 'analog'`, which is required when computing
scattering cross sections and the fission spectra. For the same reasons as the auxkernels added for post-processing, we block restrict the [!ac](MGXS) such
that they are not computed on the gap region. Finally, we select the [!ac](MGXS) that we wish to compute. This includes total cross sections (always computed),
scattering matrix cross sections (enabled by default), nu-fission cross sections (`add_fission = true`), fission chi spectra (`add_fission = true`), and fission heating cross sections (`add_fission_heating = true`). Note that other cross sections / group properties can be computed; an exhaustive list of the options can
be found in [the documentation for the MGXS block](SetupMGXSAction.md). We choose to disable the transport correction (`transport_correction = false`) for the scattering cross sections to avoid pushing the scattering ratio over unity, which can occur when there aren't enough particles scoring to higher order
scattering moments.

!listing /tutorials/lwr_mgxs/openmc_mgxs.i
  start=[Problem/MGXS]
  end=[Executioner]

Since we've selected `add_fission_heating = true`, we don't need to add other heating tallies for normalization purposes - we can
simply set `source_rate_normalization = 'kappa_fission'` in the Problem block. A steady-state executioner is selected as OpenMC will
run a single criticality calculation. We then select exodus output which will occur on the end of the
simulation.

!listing /tutorials/lwr_mgxs/openmc_mgxs.i
  block=Executioner Outputs

### Execution and Postprocessing id=exec

To run the wrapped neutronics calculation,

```bash
mpiexec -np 2 cardinal-opt -i openmc.i --n-threads=4
```

This will run OpenMC with 2 MPI ranks with 4 OpenMP threads per rank. To run the simulation faster, you can increase the parallel
processes/threads, or simply decrease the number of particles used in OpenMC. When the simulation has completed, you will have created
`openmc_out.e`: an Exodus mesh with the computed cross sections. In the console output, you should see the following tally list:

```
---------------------------------------------------------------------------------
|          Tally Name          |  Tally Score  |         AuxVariable(s)         |
---------------------------------------------------------------------------------
| MGXS_CellTally_Total_Flux    | total         | mgxs_total_g2_neutron          |
|                              |               | mgxs_total_g1_neutron          |
|                              | flux          | mgxs_flux_g2_neutron           |
|                              |               | mgxs_flux_g1_neutron           |
| MGXS_CellTally_Scatter       | nu-scatter    | mgxs_scatter_g2_gp2_l0_neutron |
|                              |               | mgxs_scatter_g2_gp1_l0_neutron |
|                              |               | mgxs_scatter_g1_gp2_l0_neutron |
|                              |               | mgxs_scatter_g1_gp1_l0_neutron |
| MGXS_CellTally_Fission       | nu-fission    | mgxs_fission_g2_gp2_neutron    |
|                              |               | mgxs_fission_g2_gp1_neutron    |
|                              |               | mgxs_fission_g1_gp2_neutron    |
|                              |               | mgxs_fission_g1_gp1_neutron    |
| MGXS_CellTally_Kappa_Fission | kappa-fission | mgxs_kappa_fission_g2_neutron  |
|                              |               | mgxs_kappa_fission_g1_neutron  |
---------------------------------------------------------------------------------
```

Each of these tallies are automatically applied to the cells discovered by Cardinal when generating the
cell to element mapping. OpenMC's native [!ac](MGXS) API does not allow for the use of distributed cell
filters when generating [!ac](MGXS), and so generating an equivalent set of cross sections would've
required the discretization of the geometry with unique cells for each region (which is incredibly time
consuming). The generated cross sections for the fuel can be found in [mgxs_xs_1], where we can see
the importance of accurately capturing spatial and spectral effects when generating group constants. The fast
neutron production cross sections for the corner pins of the LWR assembly are larger then the central pins;
this is caused by spatial shielding effects. The thermal neutron production cross section increases in the
region between the center and periphery of the assembly due to an increase in moderation. Additionally, a depression
in the thermal neutron production cross section can also be seen in the fuel pins close to the inserted control rods.

!media mgxs_xs_1.png
  id=mgxs_xs_1
  caption=The generated [!ac](MGXS) at the core centerline. Left: $\chi_{g}$. Middle: $\nu\Sigma_{f,g}$. Right: $\kappa\Sigma_{f,g}$.
  style=width:90%;margin-left:auto;margin-right:auto

The group-wise total cross section and scattering ratio for all materials at the core centerline can be found in [mgxs_xs_2].
Spatial trends are difficult to pick out due to the order-of-magnitude difference between [!ac](MGXS) for the different materials,
however we can see that the control rods are the strongest absorbers in both the fast and thermal range. The borated water
coolant has a very large scattering ratio in both the fast and thermal groups, showcasing the effectiveness of water
as a neutron moderator.

!media mgxs_xs_2.png
  id=mgxs_xs_2
  caption=The generated [!ac](MGXS) at the core centerline. Left: $\Sigma_{t,g}$. Right: $\Sigma_{s,g} / \Sigma_{t,g}$.
  style=width:60%;margin-left:auto;margin-right:auto

While not a capability available in Cardinal, we take these [!ac](MGXS) and use them as an input in a deterministic solver to showcase the end
result of a standard two-step calculations. The fast and thermal fluxes from the discrete-ordinates code Gnat ([!cite](gnat_1)) using these cross
sections can be found in [mgxs_det_g1] and [mgxs_det_g2]. The predicted continuous-energy value of $k_{eff}$
from Cardinal-OpenMC is $0.64684$. Gnat (using 200 directions per energy group) obtains a multi-group $k_{eff}$ of $0.64372$, yielding $\Delta k_{eff} = 312$
pcm. There is likely some amount of cancelation of error in the deterministic result, evidenced by the surprisingly good agreement obtained without using
transport-corrected scattering cross sections. However, we believe these results showcase the value of this automated workflow for computing spatially-varying
cross sections for deterministic transport calculations.

!media mgxs_det_g1.png
  id=mgxs_det_g1
  caption=Group 1 (fast) fluxes from Gnat using the generated [!ac](MGXS). Fluxes are normalized such that the integral over all space and energy is unity.
  style=width:75%;margin-left:auto;margin-right:auto

!media mgxs_det_g2.png
  id=mgxs_det_g2
  caption=Group 2 (thermal) fluxes from Gnat using the generated [!ac](MGXS). Fluxes are normalized such that the integral over all space and energy is unity.
  style=width:75%;margin-left:auto;margin-right:auto
